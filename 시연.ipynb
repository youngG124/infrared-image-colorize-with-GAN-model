{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "시연",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kCgAg2EAn1qjNk6xipPUCPe1FMr5m_Ar",
      "authorship_tag": "ABX9TyN7fNQRneXptAbkVY/qGiHn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngG124/infrared-image-colorize-with-GAN-model/blob/main/%EC%8B%9C%EC%97%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0D6qh_8m1Zt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "# U-NET 생성\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "        super(UNetDown, self).__init__()\n",
        "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_size))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.0):\n",
        "        super(UNetUp, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(out_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(GeneratorUNet, self).__init__()\n",
        "        \n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
        "\n",
        "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up5 = UNetUp(1024, 256)\n",
        "        self.up6 = UNetUp(512, 128)\n",
        "        self.up7 = UNetUp(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # U-Net generator with skip connections from encoder to decoder\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "        u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(u1, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1)\n",
        "\n",
        "        return self.final(u7)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalization:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatenate image and condition image by channels to produce input\n",
        "        img_input = torch.cat((img_A, img_B), 1)\n",
        "        return self.model(img_input)"
      ],
      "metadata": {
        "id": "9OBk6KOU5CJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = ''\n",
        "\n",
        "n_epochs = 100\n",
        "dataset_name = \"Night449\"\n",
        "lr = 0.0002\n",
        "b1 = 0.5                    # adam: decay of first order momentum of gradient\n",
        "b2 = 0.999                  # adam: decay of first order momentum of gradient\n",
        "decay_epoch = 100         # epoch from which to start lr decay\n",
        "#n_cpu = 8                   # number of cpu threads to use during batch generation\n",
        "channels = 3                # number of image channels\n",
        "checkpoint_interval = 20    # interval between model checkpoints\n",
        "\n",
        "\n",
        "batch_size = 12\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "\n",
        "# Loss functions\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_pixelwise = torch.nn.L1Loss()\n",
        "\n",
        "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
        "lambda_pixel = 100\n",
        "\n",
        "# Calculate output of image discriminator (PatchGAN)\n",
        "patch = (1, img_height // 2 ** 4, img_width // 2 ** 4)\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = GeneratorUNet()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "if cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_pixelwise.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "# Tensor type\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "metadata": {
        "id": "eabKWVlf49aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.load_state_dict(torch.load(\"/content/drive/MyDrive/weights_with_new_mean_std/generator_47.pth\", map_location=torch.device('cpu')))\n",
        "discriminator.load_state_dict(torch.load(\"/content/drive/MyDrive/weights_with_new_mean_std/discriminator_47.pth\", map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7E6_871UAe4",
        "outputId": "9d29bafe-e812-45dd-e0fe-a46b975910b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ffmpeg-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zvNjxSgz1aY",
        "outputId": "86310cb3-75cd-48b4-f74a-53d73468e05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "if os.path.isfile('output.mp4') :\n",
        "  os.remove('output.mp4')\n",
        "\n",
        "PREPROCESSING_DIR = '/content/drive/MyDrive/cut_images'\n",
        "if os.path.exists(PREPROCESSING_DIR) :\n",
        "  shutil.rmtree(PREPROCESSING_DIR)\n",
        "  os.makedirs(PREPROCESSING_DIR)\n",
        "else :\n",
        "  os.makedirs(PREPROCESSING_DIR)\n",
        "\n",
        "AFTER_DIR = '/content/drive/MyDrive/colored_cut_images'\n",
        "if os.path.exists(AFTER_DIR) :\n",
        "  shutil.rmtree(AFTER_DIR)\n",
        "  os.makedirs(AFTER_DIR)\n",
        "else :\n",
        "  os.makedirs(AFTER_DIR)\n",
        "\n",
        "  \n",
        "\n",
        "###########################################\n",
        "## 흑백 영상 이미지로 쪼개기\n",
        "\n",
        "# import ffmpeg\n",
        "# (\n",
        "#     ffmpeg\n",
        "#     .input(\"/content/drive/MyDrive/road.mp4\")\n",
        "#     .filter('fps', fps='24')\n",
        "#     .output('/content/drive/MyDrive/cut_images/%04d.jpg', start_number = 0)\n",
        "#     .overwrite_output()\n",
        "#     .run()\n",
        "# )\n",
        "\n",
        "\n",
        "def vid_to_img(link):\n",
        "    cam = cv2.VideoCapture(link)\n",
        "    \n",
        "    fps = cam.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists('/content/drive/MyDrive/cut_images'):\n",
        "            os.makedirs('/content/drive/MyDrive/cut_images')\n",
        "\n",
        "    except OSError:\n",
        "        print('Error : Creating directory of data')\n",
        "\n",
        "    currentframe = 0\n",
        "\n",
        "    while(True) :\n",
        "\n",
        "        ret, frame = cam.read()\n",
        "\n",
        "        if (ret is True):\n",
        "\n",
        "            # 비디오가 이미지 계속 생성, 이름은 네자리 정수\n",
        "            name = '/content/drive/MyDrive/cut_images/' + str(\"%04d\"% currentframe) + '.jpg'\n",
        "            if (currentframe % 50 == 0) :\n",
        "                print('전처리중...' + name)            \n",
        "\n",
        "            # 추출된 이미지 쓰기\n",
        "            cv2.imwrite(name, frame)\n",
        "\n",
        "            # 다음프레임\n",
        "            currentframe += 1\n",
        "\n",
        "            if cv2.waitKey(1) > 0 :\n",
        "                break\n",
        "        else :\n",
        "            break\n",
        "\n",
        "    cam.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"총 프레임 : \" + str(currentframe))\n",
        "\n",
        "    return int(fps)\n",
        "\n",
        "\n",
        "video_fps = vid_to_img(\"/content/drive/MyDrive/g.mp4\")\n",
        "\n",
        "print(\"fps : \" + str(video_fps))\n",
        "\n",
        "\n",
        "#################################\n",
        "# 저장된 흑백 이미지들 셋에 싣기\n",
        "\n",
        "class NightTestDataset(Dataset):\n",
        "    def __init__(self, root, color_transforms_=None, gray_transforms_=None):\n",
        "\n",
        "        self.color_transforms = transforms.Compose(color_transforms_)\n",
        "        self.gray_transforms = transforms.Compose(gray_transforms_)\n",
        "        self.gray_files = sorted(glob.glob(os.path.join(root, '/content/drive/MyDrive/cut_images') + \"/*.*\"))\n",
        "        self.color_files = sorted(glob.glob(os.path.join(root, '/content/drive/MyDrive/cut_images') + \"/*.*\"))\n",
        "     \n",
        "    def __getitem__(self, index):\n",
        "        gray_img = Image.open(self.gray_files[index % len(self.gray_files)]).convert(\"RGB\")\n",
        "        color_img = Image.open(self.color_files[index % len(self.color_files)]).convert(\"RGB\")\n",
        "    \n",
        "        gray_img = self.gray_transforms(gray_img)\n",
        "        color_img = self.color_transforms(color_img)\n",
        "\n",
        "        return {\"A\": gray_img}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.gray_files)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "color_mean = [0.485, 0.456, 0.406]\n",
        "color_std = [0.229, 0.224, 0.225]\n",
        "gray_mean = [ 0.44,0.44,0.44]\n",
        "gray_std = [ 0.26,0.26,0.26]\n",
        "\n",
        "color_transforms_ = [\n",
        "                     transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=color_mean, std=color_std),\n",
        "]\n",
        "\n",
        "gray_transforms_ = [\n",
        "                     transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "                    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Normalize(mean=gray_mean, std=gray_std),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "test_root = root + '/content/drive/MyDrive/cut_images'\n",
        "test_batch_size = 6\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    NightTestDataset(test_root, color_transforms_=color_transforms_, gray_transforms_=gray_transforms_),\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "def reNormalize(img, mean, std):\n",
        "    img = img.numpy().transpose(1, 2, 0)\n",
        "    img = img * std + mean\n",
        "    img = img.clip(0, 1)\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################################\n",
        "# eval모드, 이미지 채색해서 저장\n",
        "\n",
        "generator.eval()\n",
        "discriminator.eval()\n",
        "\n",
        "from skimage import *\n",
        "from skimage import io\n",
        "\n",
        "def test_images(epoch, loader, mode):\n",
        "\n",
        "    c = 0\n",
        "    for i, batch in enumerate(loader) :\n",
        "       gray = Variable(batch[\"A\"].type(Tensor))\n",
        "       output = generator(gray)\n",
        "       for j in range(len(output)) :\n",
        "          out_img = np.transpose(output.cpu().data.numpy()[j],(1,2,0))\n",
        "          io.imsave(\"/content/drive/MyDrive/colored_cut_images/\" + \"%04d\"%(c*6+j)  + \".jpg\" , img_as_ubyte(out_img))\n",
        "          if (c*6+j % 50 == 0) :\n",
        "                print('후처리중...' + str(c*6+j) +'/'+ )    \n",
        "       c += 1\n",
        "\n",
        "test_images(n_epochs, test_loader, 'test')\n",
        "\n",
        "\n",
        "##############################\n",
        "# 채색된 이미지들 mp4로 output\n",
        "\n",
        "import ffmpeg\n",
        "(\n",
        "    ffmpeg\n",
        "    .input('/content/drive/MyDrive/colored_cut_images/*.jpg', pattern_type='glob', framerate=video_fps)\n",
        "    .output('output.mp4')\n",
        "    .run()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsiRc0ruUBsj",
        "outputId": "975abf60-c87f-41f5-f8a9-3bdc2c2de5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리중.../content/drive/MyDrive/cut_images/0000.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0050.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0100.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0150.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0200.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0250.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0300.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0350.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0400.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0450.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0500.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0550.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0600.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0650.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0700.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0750.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0800.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0850.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0900.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/0950.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1000.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1050.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1100.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1150.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1200.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1250.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1300.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1350.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1400.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1450.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1500.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1550.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1600.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1650.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1700.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1750.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1800.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1850.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1900.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/1950.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2000.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2050.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2100.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2150.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2200.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2250.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2300.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2350.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2400.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2450.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2500.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2550.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2600.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2650.jpg\n",
            "전처리중.../content/drive/MyDrive/cut_images/2700.jpg\n",
            "총 프레임 : 2740\n",
            "25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}